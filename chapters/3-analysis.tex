\chapter{Analysis}
\label{chap:analysis}
This chapter studies a wide range of \gls{PETs}, discussing how they work and analyzing whether they are applicable in the context of a decentralized middleware for enhancing data privacy in Solid. Based on this analysis, a number of \gls{PETs} will be chosen to be supported in \middleware{}. 

\section{Transformation-based approaches}
\label{sec:transformation-approaches}
Transformation-based approaches are approaches that modify datasets in a uniform way (on a by-row basis), such that the resulting dataset contains less \gls{PII}. Since data transformations \textit{modify} the dataset, it is important to note that this has an impact on the utility of the data. Some transformations may be more powerful at anonymizing the data, but at the same time make the data less valuable or even unusable. A balance must be struck between data privacy and utility, where the choice for a certain data transformation is very context-dependent. This section introduces a number of data transformations.

For an overview of common architectural tactics for data de-identification, we look at the literature study by \citeauthor{de-id-taxonomy}. This study includes 163 investigated articles and builds a taxonomy for data de-identification architectural tactics, mostly consisting of transformation-based approaches. Table \ref{table:de-id-taxonomy} presents their proposed data transformation tactics.

Such transformations seem very usable in the context of a middleware that anonymizes data. They are often fairly simple transformations, that can range in privacy strength and resulting utility. Because these transformations are so dynamic, they are perfect to adapt to different trust levels of applications, allowing more impactful transformations on data sent to applications that are less trusted. Another big advantage of an approach taking advantage of data transformations is that this can be realised statelessly. As the data is sanitized on a record-by-record basis, independently of the other records, there is no need to keep track of state. This heavily simplifies possible architectural designs.
\newpage
\input{tables/deid-transformations}

\section{Encrypted Database Systems}\todo{Can I just import table like this (plagiarism/copyright)?}\todo{Do generalisations, such as going from city to province, fall under aggregate?}
Encrypted Database Systems are database systems that store encrypted data, while maintaining some functionality over this data. It is a vast research field, with \citet{sok-cryptdb} being a formative work. The need for encrypted database systems arises from the rise of the PaaS\footnote{Platform-as-a-Service} model, whereby software and database deployment is outsourced to third parties such as Amazon Web Services and others. Since these "cloud platforms" store sensitive data (customer data, economically valuable information, ...), it is important for certain users to guarantee that these providers are unable to access their stored data. This is realised by encrypting the data, but this brings along problems. Since these databases are often very large, information is retrieved through complex queries. These queries may not support operating on encrypted data by default, so some new techniques are necessary.

Many types of encryption exist for these database systems, ranging from very secure to less secure, but in turn providing more functionality. Popular encryption schemes, sorted from least to most leakage, are \citep{cryptdice}:
\begin{itemize}
    \item \textbf{Random/probabilistic encryption}: Random encryption is an encryption scheme in which encrypting the same plaintext results in a different ciphertext. This is usually achieved by making use of the AES encryption scheme, and using a randomly generated Initialization Vector (IV). This scheme provides the strongest data protection, but it offers no useful operations on the data.
    \item \textbf{Deterministic encryption}: Deterministic encryption works similar to random encryption, but now the IV is kept constant. This results in a scheme which gives the same ciphertext for the same plaintext input. This is less secure, but allows for equality comparisons.
    \item \textbf{Order-preserving encryption}: In the order-preserving encryption scheme, the order relationship between plaintext is preserved in the ciphertext. Therefore, it is possible to do order-comparisons between encrypted data elements, allowing functionality such as range queries.
    \item \textbf{Order-revealing encryption}: Order-revealing encryption is an outdated form of order-preserving encryption. Its usage is discouraged, since it has a larger leakage then order-revealing encryption while providing the same level of functionality. \todo{Add citation here}
\end{itemize}
A new type of encryption scheme that has recently started to become popular is \textit{homomorphic encryption}: it allows processing on encrypted data by supporting mathematical operations on the encrypted data, such as multiplication and addition. It has become popular over the last decade, with \citet{fhe} being a breakthrough work and Google recently releasing an open-source library\footnote{\url{https://github.com/google/fully-homomorphic-encryption}} supporting fully homomorphic encryption. While Somewhat Homomorphic Encryption already has practical applications, this is less the case for Fully Homomorphic Encryption, with the main hurdle being that it is very computationally intensive.
\begin{itemize}
    \item \textbf{Somewhat Homomorphic Encryption}: Somewhat homomorphic encryption schemes, also called partially homomorphic encryption schemes, are homomorphic encryption schemes that support either addition or multiplication, but not both \citep{she}. An example of a additive homomorphic encryption scheme is the Paillier Cryptosystem \citep{paillier}. A widely used multiplicative homomorphic encryption scheme is the ElGamal CryptoSystem \citep{elgamal}.
    \item \textbf{Fully Homomorphic Encryption}: Fully homomorphic encryption schemes are homomorphic encryption schemes that support both addition and multiplication on encrypted data. This is incredibly powerful, since any function can be represented as a circuit consisting of multiplication and addition gates. It can, theoretically, thus compute any function over encrypted data. However, it is still too computationally intensive for a number of practical applications \citep{he-practical, pragmatic-mpc}. Despite that hurdle it remains a promising technology, allowing applications such as encrypted search queries \citep{fhe}, and every year more efficient implementations are proposed.
\end{itemize}

\noindent While Encrypted Database Systems form a very active research domain, it is less applicable when the focus is on giving data to untrusted applications. These applications need the data by definition, and the goal is to give them modified datasets resulting in less leakage. In contrast, Encrypted Databases are applicable when the \textit{storage provider} is untrusted, instead of the application. This is especially the case for non-homomorphic encryption. 

Homomorphic encryption is also not applicable in the scope of what this thesis researches, however, it may be interesting future work. Homomorphic encryption could be used to give some encrypted data to an application that then performs computations on it. These computations could then be returned to the component that encrypted them, decrypting the results before storing them in the Solid server again. This way, secure computations on data could be supported in the Solid ecosystem.\todo{Reference as future work!}

\section{Secure Multi-Party Computation}
\label{sec:mpc}
\gls{MPC} is an exciting research field that has seen a large amount of research over the last few decades, while the techniques and modern computers have only recently evolved enough to allow practical applications. It is a technique that allows any number of parties to secretly compute any function over some encrypted input data, while only revealing the requested output to the parties. The basic techniques for Multi-Party Computation stem from a groundbreaking paper by \citet{yao} (for two-party computation), which was later extended to the multi-party case by \citet{mpc}.

\gls{MPC} has a somewhat unusual security model, since the adversary can be a party to the computation instead of being an outsider. The adversary model thus considers some adversarial entity, which manages to exert control over a subset of the parties partaking in the computation. Such a controlled party is called \textit{corrupted}. Often, a \textit{monolithic adversary} is considered: if there are multiple corrupted parties, it is assumed they work together \citep{mpc-good-practice}. When an adversary controls multiple corrupted parties, several \textit{corruptions strategies} are possible \citep{secure-mpc}. In the \textit{static corruption model}, the set of corrupted parties is fixed from the onset and does not change during execution of the protocol. In the \textit{adaptive corruption model}, the adversary can corrupt parties during the execution, but once a party has been corrupted it remains corrupted until the end of the protocol. Finally, there is the \textit{proactive security model} \citep{sec-transient-failures}, where corrupted parties can become honest again (for example, because a breach has been detected and the systems are recovered).\todo{Too detailed?}

Of course, many protocols exist for facilitating computation between multiple parties, even HTTP would support that. The goal of \gls{MPC} is to provide a \textit{secure} protocol. Often, the \textit{real-ideal}\footnote{In the real-ideal security definition, an adversary can not harm the real protocol more than what would be possible in an ideal protocol \citep{mpc}} definition of security is taken to define when such a system is secure. However, this does not provide many practical guidelines. \citet{secure-mpc} lists a number of minimal (but not exhaustive) requirements, which every secure protocol should fulfill:

\begin{quote}{\citet[p.2]{secure-mpc}}
\begin{enumerate}
    \item \textbf{Privacy}: No party should learn anything more than its prescribed output.
    \item \textbf{Correctness}: Each party is guaranteed that the output it receives is correct.
    \item \textbf{Independence of Inputs}: Corrupted parties must choose their inputs independently of the honest parties' inputs.
    \item \textbf{Guaranteed Output Delivery}: Corrupted parties should not be able to prevent honest parties from receiving their output.
    \item \textbf{Fairness}: Corrupted parties should receive their outputs if and only if the honest parties also receive their outputs.
\end{enumerate}
\end{quote}
The actual computation, then, is usually based on something called a \textit{garbled circuit}, introduced by \citeauthor{yao}\todo{add citation for garbled circuits}. The function to be computed is represented as a boolean circuit where the gates are encrypted. \todo{source}The explanation of the complete protocol is out of the scope for this thesis, but it is described in \citet{secure-mpc, pragmatic-mpc} and uses techniques such as Oblivious Transfer\todo{cite}. For some use-cases, generic \gls{MPC} protocols may not be the most efficient and specific protocols that are much faster may exist. A very common example is Private Set Intersection\todo{cite, add acronym}.

\todo[inline]{Add explanation why MPC is not suitable for our middleware}

\section{Differential Privacy \& \textit{k}-Anonymity}
Differential privacy and \textit{k}-Anonymity are both types of \textit{statistical} privacy. 

In differential privacy, \textit{"participating in a database does not substantially increase the risk to the user's privacy"} \citep{diff-privacy}. \todo{Look later at this paper, it provides some references to proofs that non-interactive (ie data transformations) work less well than interactive approaches, because at the time of the sanitization the future utility is not always known yet. Then discuss that statistical methods cannot be used since we only have data from one user.} This definition follows from \citeauthor{diff-privacy}'s impossibility proof of absolute disclosure prevention. She proved that even if a database has very little information (such as the average height of a person for every country), even that information can lead to a privacy breach in the presence of side information. Imagine that a person's height is sensitive information, and that it is known that Alice's height is 2cm less than the average height. Then the database containing average heights discloses Alice's height, leading to a privacy breach. Because this is impossible to circumvent, \citeauthor{diff-privacy} introduces the relative notion: a disclosure is just as likely whether the individual takes part in the database or not. In short, the technique works by adding specifically chosen random noise to the answer of a query, to statistically provide the \textit{$\epsilon$}-differential privacy guarantees. A more rigorous explanation can be found in \citet[p9-11]{diff-privacy}.

\textit{k}-Anonymity was introduced by \citet{k-anonymity}, who defined it in terms of de-identifying persons from so-called \textit{quasi-identifiers} (i.e. indirect identifiers) in released data sets. \citet{demographics-identify-unique} notes that 53\% of the U.S. population can be uniquely identified with only the following attributes: place, gender and date of birth. In this context, it is very difficult to correctly identify indirect identifiers, since it is not known beforehand with which data this can be combined to determine unique identities. If a dataset is \textit{k}-anonymous, however, it is impossible (even when combining with another dataset), to uniquely identify individuals. A data release is \textit{k}-anonymous if "the information for each person contained in the release cannot be distinguished from at least \textit{k}-1 individuals whose information also appears in the release" \citep{k-anonymity}. Thus, every unique combination of identifiers maps to at least \textit{k} individuals, making individual identification unlikely\footnote{There exist attacks on \textit{k}-anonymity, but most can be resisted by following the accompanying policies stipulated in \citet{k-anonymity}.}.

\todo[inline]{Add better explanation using the class from Privacy-Preserving Technologies}

While both technologies are very interesting, there are a number of crucial aspects that stop them from being usable in \middleware{}. First of all, both methods are applicable in datasets with \textit{multiple} users. There is no point in using these methods on data originating from a single user. For differential privacy, this is because it is defined as your participation in the database having a negligible impact (defined by $\epsilon$). However, when the database only consists of records belonging to a single user, this is impossible. On the other hand, \textit{k}-Anonymity lacks that it focuses on a different aspect than what \middleware{} tries to achieve. This is because \textit{k}-Anonymity focuses on preventing \textit{identity disclosure}, while \middleware{} aims to lower the risks of \textit{attribute disclosure}. 

\section{Related works}\todo{Is this section useful? Or does the general text already give enough references to related works? And where does it belong, in the beginning or at the end?}
\noindent \textbf{CryptDB} \citep{cryptdb} is a seminal work in the research field of encrypted database systems. It is a SQL database that allows queries over encrypted data. Its main contributions are the demonstration of two new techniques, \textit{adjustable query-based encryption} and \textit{encryption key chaining}. 

Adjustable query-based encryption is a technique that dynamically adapts the the encryption scheme of columns based on queries: it automatically selects the highest possible security given the queries that need to be executed on that column. It does this in a two-fold manner. Firstly, different columns (and tables) have different encryption keys, so that the encryption level can be varied on data fields (some columns might be more sensitive and thus require stronger encryption). Secondly, CryptDB encrypts columns in an \textit{onion}: when the database is initialized, every column is encrypted several times, each time with a stronger encryption mechanism. When a query is then executed against a column whose strong encryption scheme is not compatible with the query, layers are removed until a compatible encryption scheme is reached. 

Encryption key chaining then, is a method to cryptographically enforce access control policies. In CryptDB, principals (either users or groups of users) all have their own key. \citeauthor{cryptdb} developed a novel way to encrypt data such that only the correct principals have access to the data, by making use of both symmetric and asymmetric keys: they use symmetric keys when both users are online, so an interim key can be stored in memory, or asymmetric encryption when one of the users is not online. This strikes a balance between security (only current keys of online users can be leaked in case of an attack) and performance (as symmetric encryption is much more performant). The technique itself requires some background information and can thus be found in the paper itself.\todo{This paragraph does not bring over the idea well, so should be rewritten later}\\

\noindent \textbf{DataBlinder}, by \citet{datablinder}, presents ... \todo{Write more about related works}\\

\noindent \textbf{Cryptdice} \citep{cryptdice} illustrate ...\\

\noindent \textbf{Secure Multi-Party Computation} \citep{secure-mpc} gives a good overview of\\

