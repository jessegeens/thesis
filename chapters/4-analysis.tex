\chapter{Study of Privacy-enhancing Technologies}
\label{cha:analysis}
This chapter studies a wide range of \gls{PETs}, discussing how they work and analyzing whether they are applicable in the context of enhancing data privacy in Solid. As explained in section \ref{sec:outline}, a component for enhancing privacy is first studied and developed. This analysis forms the basis for selecting a number of \gls{PETs} that will be supported in the privacy-enhancing component of \middleware{}, the developed aggregation middleware. 

Section \ref{sec:pets} starts with an introduction to the concept of \gls{PETs}, explaining why this kind of technology is needed and what the basic concepts are. The next sections will each treat one specific PET. First, transformation-based approaches are handled by section \ref{sec:transformation-approaches}. Section \ref{sec:enc-db} then continues with encrypted databases, which is followed by \acrlong{MPC}. Section \ref{sec:statistical-privacy} handles two types of statistical privacy, and the final section discusses \acrlong{ABE}.

\section{Introduction}
\label{sec:pets}
This section introduces the notion of \gls{PETs}, starting with an explanation of their necessity. Next, this section presents some of the basic concepts of \gls{PETs}, which come back in the later sections where concrete technologies are discussed.
\subsection{The need for Privacy-Enhancing Technologies}
The concept of privacy has been around for a long time. Even in the 14th century already, cases were brought before court for eavesdropping and opening letters \citep{privacy-history}. Initially, it was more understood in the context of being let alone, especially in one's house or personal properties. Since the dawn of the information age after the Second World War, the concept started to shift more towards privacy in the context of personal information and publications on the topic started to appear. The definition did not change much however, remaining very similar to the one introduced in 1891 by Warren and Brandeis \citep{privacy-history}:
\newpage
\begin{quote}{Samuel Warren, Louis Brandeis}
    ``Privacy is described as a right to be let alone and a right of each individual  to  determine,  under  ordinary  circumstances,  what  his  or  her  thoughts, sentiments, and emotions shall be when in communication with others.''
\end{quote}
\noindent The ubiquity of personal data collection in recent years has strengthened privacy concerns. Especially since the Web has become extremely centralized, with only six companies accounting for 43\% of global internet traffic \citep{internet-report}, there has been a growing demand for technologies that help protect user privacy.

Not only from the \textit{Data Subject}'s perspective is there a growing concern for data privacy. With the introduction of the \gls{GDPR} in 2018, companies have a legal obligation to take care of user data, especially if it is sensitive. This clashes with the rise of the Software- and Platform-as-a-service (Saas/PaaS) business models, where companies outsource data storage and management. However, this is not always possible and comes with risks. If a \textit{Data Processor}\footnote{Data Subject, Data Processor and Data Controller are roles defined in the \gls{GDPR}. For a concise but correct definition, please refer to \url{https://ec.europa.eu/info/law/law-topic/data-protection/reform/rules-business-and-organisations/obligations/controller-processor/what-data-controller-or-data-processor_en}} (a third party that processes data on behalf of a Data Controller) leaks data, the \textit{Data Controller} risks violating the user's trust (and legal sanctions).

In both cases, Privacy-Enhancing Technologies play a big role in reducing the risk of data leaks and preserving user privacy. \citeauthor{pets-handbook} introduced the following definition, which was later adopted by the European Commission\footnote{\url{https://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=COM:2007:0228:FIN:EN:PDF}}:
\begin{quote}{\citeauthor{pets-handbook}}
    ``Privacy-Enhancing Technologies is a system of ICT measures protecting informational privacy by eliminating or minimising personal data thereby preventing unnecessary or unwanted processing of personal data, without the loss of the functionality of the information system.''
\end{quote} 
%The rest of this chapter will introduce a number of widely used \gls{PETs}, some of which will be used in \middleware{}.

\subsection{Data (de-)identification}
\label{sec:data-deid}
The previous section gave an introduction to the concept of privacy. In IT systems, this applies to personal data. But when is data personal? In the context of ePrivacy, the term \textit{\gls{PII}}, is used widely. The term applies to data elements that contain references to attributes that can, either directly or indirectly, disclose someone's identity. A distinction can then be made between three types of data \citep{de-id-taxonomy}:
\begin{itemize}
    \item \textbf{Direct identifiers}: data attributes that directly identify an individual. Examples are national identification numbers, home addresses, e-mail addresses, full names, etc.
    \item \textbf{Indirect identifiers}: also called quasi-identifiers, these are data attributes that can be combined with other data attributes to identify an individual, but do not disclose an identity on their own. Examples are birth dates, family names, IP addresses etc.
    \item \textbf{Non-PII elements}: data attributes that contain no reference at all to a person's identity.
\end{itemize}
\noindent \citeauthor{privacy-design-strategies} notes that much like security, privacy is a core property of software systems, and is heavily architecture-dependent. It is therefore of paramount importance that privacy considerations are taken into account from the outset and through the whole development cycle. This design philosophy is named \textit{privacy by design} and it is one of the requirements listed in the GDPR. For regular software design, software developers have used software design patterns to solve common architectural problems. Likewise, \citet{privacy-design-strategies} and \citet{de-id-taxonomy} propose a number of \textit{privacy design strategies}, which will be used extensively.

\section{Transformation-based approaches}
\label{sec:transformation-approaches}
Transformation-based approaches are approaches that modify datasets in a way such that the resulting dataset contains less \gls{PII}. The transformation-based approaches studied  in this section all happen on a row-by-row basis. Since data transformations \textit{modify} the dataset, it is important to note that this has an impact on the utility of the data. Some transformations may be more powerful at anonymizing the data, but at the same time make the data less valuable or even unusable. A balance must be struck between data privacy and utility, where the choice for a certain data transformation is very context-dependent. This section introduces a number of data transformations.

For an overview of common architectural tactics for data de-identification, we look at a literature study by \citeauthor{de-id-taxonomy}. This study includes 163 investigated articles and builds a taxonomy for data de-identification architectural tactics, mostly consisting of transformation-based approaches. Table \ref{table:de-id-taxonomy} presents their proposed data transformation tactics.

Such transformations seem very usable in the context of a middleware that anonymizes data. They are often fairly simple transformations, that can range in privacy strength and resulting utility. Because these transformations are so dynamic, they are perfect to adapt to different trust levels of applications, allowing more impactful transformations on data sent to applications that are less trusted. Another big advantage of an approach taking advantage of data transformations is that this can be realised statelessly. As the data is sanitized on a row-by-row or record-by-record basis, independently of the other records, there is no need to keep track of state. This heavily simplifies possible architectural designs.
\newpage
\input{tables/deid-transformations}

\section{Encrypted Database Systems}
\label{sec:enc-db}
\noindent Encrypted Database Systems are database systems that store encrypted data, while maintaining some functionality over this data. It is a vast research field, with \citet{sok-cryptdb} being a formative work. The need for encrypted database systems arises from the rise of the PaaS model, whereby software and database deployment is outsourced to third parties such as Amazon Web Services and others. Since these cloud platforms store sensitive data (customer data, economically valuable information, ...), it is important for certain users to guarantee that these providers are unable to access their stored data. This is realized by encrypting the data, but this brings along problems. As these databases are often very large, information is retrieved through complex queries. These queries may not support operating on encrypted data by default, so some new techniques are necessary \citep{datablinder}. 

Many types of encryption exist for these database systems, ranging from very secure to less secure, but in turn providing more functionality. Popular encryption schemes, sorted from least to most leakage, are \citep{cryptdice, datablinder}:
\begin{itemize}
    \item \textbf{Random/probabilistic encryption}: Random encryption is an encryption scheme in which encrypting the same plaintext results in a different ciphertext. This is usually achieved by making use of the AES encryption scheme, and using a randomly generated \gls{IV}. This scheme provides the strongest data protection, but it offers no useful operations on the data.
    \item \textbf{Deterministic encryption}: Deterministic encryption works similar to random encryption, but now the \gls{IV} is kept constant. This results in a scheme which gives the same ciphertext for the same plaintext input. This is less secure, but allows for equality comparisons.
    \item \textbf{Order-revealing encryption}: In the order-revealing encryption scheme, the order relationship between plaintext is preserved in the ciphertext. Therefore, it is possible to do order-comparisons between encrypted data elements, allowing functionality such as range queries.
    \item \textbf{Order-preserving encryption}: Order-preserving encryption is an outdated form of order-revealing encryption. Its usage is discouraged, since it has a larger leakage than order-revealing encryption while providing the same level of functionality \citep{stanford-ore}.
\end{itemize}
A new type of encryption scheme that has recently started to become popular is \textit{homomorphic encryption}: it allows processing on encrypted data by supporting mathematical operations on the encrypted data, such as multiplication and addition. It has become popular over the last decade, with \citet{fhe} being a breakthrough work and Google recently releasing an open-source library\footnote{\url{https://github.com/google/fully-homomorphic-encryption}} supporting fully homomorphic encryption. While Somewhat Homomorphic Encryption already has practical applications, this is less the case for Fully Homomorphic Encryption, with the main hurdle being that it is very computationally intensive.
\begin{itemize}
    \item \textbf{Somewhat Homomorphic Encryption}: Somewhat homomorphic encryption schemes, also called partially homomorphic encryption schemes, are homomorphic encryption schemes that support either addition or multiplication, but not both \citep{she}. An example of a additive homomorphic encryption scheme is the Paillier Cryptosystem \citep{paillier}. A widely used multiplicative homomorphic encryption scheme is the ElGamal Cryptosystem \citep{elgamal}.
    \item \textbf{Fully Homomorphic Encryption}: Fully homomorphic encryption schemes are homomorphic encryption schemes that support both addition and multiplication on encrypted data. This is incredibly powerful, since any function can be represented as a circuit consisting of multiplication and addition gates. It can, theoretically, thus compute any function over encrypted data. However, it is still too computationally intensive for a number of practical applications \citep{he-practical, pragmatic-mpc}. Despite that hurdle it remains a promising technology, allowing applications such as encrypted search queries \citep{fhe}, and every year more efficient implementations are proposed.
\end{itemize}

\noindent While Encrypted Database Systems form a very active research domain, their possible applications within Solid are rather limited for a number of reasons. First and foremost, Solid is not a database system and does not provide data with support for queries (at the moment). Instead, data fetched through the Solid protocol is in the form of resources, which map to files on a filesystem. In the future, Solid may support RDF queries on the resources, but since this is rather specialized implementing such a system would also be a complex task. The second and most important point is that these encryption systems are ideal for when a single client needs access to a database. However, this maps badly to Solid's decentralized nature, where multiple clients (which are trusted differently and have different access control rights) access multiple Solid pods. This complicates the encryption possibilities substantially, especially regarding key management. 

Homomorphic encryption does have some interesting properties that would benefit from being researched in the context of Solid. Homomorphic encryption could be used to give some encrypted data to an application that then performs computations on it. In this way, a client application could aggregate data from a number of different pods, performing some calculation on it, and then returning this data to the clients, giving them insights. This scenario is further developed as future work in section \ref{sec:future-work} under FW \ref{fw:homomorphic-encryption}. This fits well in the goals of this dissertation, but then focused on the software that performs the aggregation instead of the middleware.

\section{Secure Multi-Party Computation}
\label{sec:mpc}
\gls{MPC} is an exciting research field that has seen a large amount of research over the last few decades, while the techniques and modern computers have only recently evolved enough to allow practical applications. It is a technique that allows any number of parties to secretly compute any function over some encrypted input data, while only revealing the requested output to the parties. The basic techniques for Multi-Party Computation stem from a groundbreaking paper by \citet{yao} (for two-party computation), which was later improved to support the multi-party case by \citet{mpc}.

\gls{MPC} has a somewhat unusual security model, since the adversary can be a party to the computation instead of being an outsider. The adversary model thus considers some adversarial entity, which manages to exert control over a subset of the parties partaking in the computation. Such a controlled party is called \textit{corrupted}. Often, a \textit{monolithic adversary} is considered: if there are multiple corrupted parties, it is assumed they work together \citep{mpc-good-practice}. When an adversary controls multiple corrupted parties, several \textit{corruptions strategies} are possible \citep{secure-mpc}. In the \textit{static corruption model}, the set of corrupted parties is fixed from the onset and does not change during execution of the protocol. In the \textit{adaptive corruption model}, the adversary can corrupt parties during the execution, but once a party has been corrupted it remains corrupted until the end of the protocol. Finally, there is the \textit{proactive security model} \citep{sec-transient-failures}, where corrupted parties can become honest again (for example, because a breach has been detected and the systems are recovered).

Of course, many protocols exist for facilitating computation between multiple parties, even HTTP would support that. The goal of \gls{MPC} is to provide a \textit{secure} protocol. Often, the \textit{real-ideal}\footnote{In the real-ideal security definition, an adversary can not harm the real protocol more than what would be possible in an ideal protocol \citep{mpc}} definition of security is taken to define when such a system is secure. However, this does not provide many practical guidelines. \citet{secure-mpc} lists a number of minimal (but not exhaustive) requirements, which every secure protocol should fulfill:

\begin{quote}{\citet[p.2]{secure-mpc}}
\begin{enumerate}
    \item \textbf{Privacy}: No party should learn anything more than its prescribed output.
    \item \textbf{Correctness}: Each party is guaranteed that the output it receives is correct.
    \item \textbf{Independence of Inputs}: Corrupted parties must choose their inputs independently of the honest parties' inputs.
    \item \textbf{Guaranteed Output Delivery}: Corrupted parties should not be able to prevent honest parties from receiving their output.
    \item \textbf{Fairness}: Corrupted parties should receive their outputs if and only if the honest parties also receive their outputs.
\end{enumerate}
\end{quote}
The actual computation, then, is usually based on something called a \textit{garbled circuit} \citep{garbled-circuit}. The function to be computed is represented as a boolean circuit whose gates are encrypted. The explanation of the complete protocol is out of scope for this dissertation, but it is described in \citet{secure-mpc, pragmatic-mpc}. For some use-cases, generic \gls{MPC} protocols may not be the most efficient and specific protocols that are much faster may exist. A very common example is Private Set Intersection.

Because of the decentralized nature of \gls{MPC}, this forms a very interesting technique to research in Solid. It could be used to securely compute insights over data stored in multiple pods, without even needing to hand out an encrypted version of the data. Computations are performed on the pods itself, and only \textit{secret shares} are sent out\footnote{A secret share is a part of the data that is indistinguishable from a random string. It is used in \gls{MPC} to perform the computations. When $x$ is a confidential piece of data, you could build two secret shares by generating a random $p$ and handing out $x + p$ and $p$. Only by combining the two secret shares can the original secret $x$ be reconstructed}. While this is very interesting, it also entails a very large extension of the Solid protocol or a Solid server. Moreover, this redefines the concept of a pod, making it also a compute engine instead of just a storage space. This scenario is also considered future research and is elaborated in section \ref{sec:future-work} under FW \ref{fw:mpc}.

\section{Differential Privacy \& \textit{k}-Anonymity}
\label{sec:statistical-privacy}
Differential privacy and \textit{k}-Anonymity are both types of \textit{statistical} privacy. In differential privacy, \textit{``participating in a database does not substantially increase the risk to the user's privacy''} \citep{diff-privacy}.  This definition follows from \citeauthor{diff-privacy}'s impossibility proof of absolute disclosure prevention. She proved that even if a database has very little information (such as the average height of a person for every country), even that information can lead to a privacy breach in the presence of side information. Imagine that a person's height is sensitive information, and that it is known that Alice's height is 2cm less than the average height. Then the database containing average heights discloses Alice's height, leading to a privacy breach. Because this is impossible to circumvent, \citeauthor{diff-privacy} introduces the relative notion: a disclosure is just as likely whether the individual takes part in the database or not. In short, the technique works by adding specifically chosen random noise to the answer of a query, to statistically provide the \textit{$\epsilon$}-differential privacy guarantees. A more rigorous explanation can be found in \citet[p9-11]{diff-privacy}.

\textit{k}-Anonymity was introduced by \citet{k-anonymity}, who defined it in terms of de-identifying persons from so-called \textit{quasi-identifiers} (i.e. indirect identifiers) in released data sets. \citet{demographics-identify-unique} notes that 53\% of the U.S. population can be uniquely identified with only the following attributes: place, gender and date of birth. In this context, it is very difficult to correctly identify indirect identifiers, since it is not known beforehand with which data this can be combined to determine unique identities. If a dataset is \textit{k}-anonymous, however, it is very hard to uniquely identify individuals (given that no other dataset is released - see footnote \ref{foot:k-anon-attacks}). A data release is \textit{k}-anonymous if "the information for each person contained in the release cannot be distinguished from at least \textit{k}-1 individuals whose information also appears in the release" \citep{k-anonymity}. Thus, every unique combination of identifiers maps to at least \textit{k} individuals, making individual identification unlikely\footnote{\label{foot:k-anon-attacks}There exist attacks on \textit{k}-anonymity, but most can be resisted by following the accompanying policies stipulated in \citet{k-anonymity}. Most of these attacks stem from the fact that $k$-Anonymity does not compose: combining multiple $k$-Anonymous datasets does not lead to a combined dataset that is also $k$-Anonymous.}.

While both technologies are very interesting, there are a number of crucial aspects that stop them from being usable in the privacy-enhancing middleware this dissertation aims to develop. First of all, both methods are applicable in datasets with \textit{multiple} users. There is no point in using these methods on data originating from a single user. For differential privacy, this is because it is defined as your participation in the database having a negligible impact (defined by $\epsilon$). However, when the database only consists of records belonging to a single user, this is impossible. On the other hand, \textit{k}-Anonymity lacks that it focuses on a different aspect than what the middleware tries to achieve. This is because \textit{k}-Anonymity focuses on preventing \textit{identity disclosure}, while in the middleware itself the goals are to lower the risks of \textit{attribute disclosure}. However, \textit{k}-Anonymity may be an interesting approach to prevent identity disclosure on an aggregation system (i.e. the component that collects data across many pods, or the \texttt{Aggregator} in figure \ref{fig:aggregation-flow}).

\section{Attribute-based Encryption}
\Gls{ABE} is an encryption scheme that was introduced by \citet*{fuzzy-ibe}. In their paper, the authors developed a new type of \gls{IBE}. Traditional \gls{IBE} systems typically use a single string to represent a user's identity; \citeauthor*{fuzzy-ibe} developed a scheme where the identity is not represented by a string but by a set of descriptive attributes. In their scheme, encrypting documents happens with a set of required attributes included. Decryption can then only happen by users whose identity contains all the attributes (or, where there is a big enough set overlap, in their original scheme). However, \citeauthor*{fuzzy-ibe}'s development focused mostly on using \gls{IBE} with biometric identities. A number of follow-up papers extended this idea to make it more suitable for \gls{ABE}, using systems such as \gls{KP-ABE} and \gls{CP-ABE}.

\citet{kp-abe} extends the original paper with \gls{KP-ABE} and makes it more suitable for fine-grained access control. They do this by introducing a new access structure based on a tree consisting of \texttt{AND} and \texttt{OR} gates\footnote{Actually, the tree consists of nodes that act as threshold gates. \texttt{AND} gates are realised as n-out-of-n threshold gates, while \texttt{OR} gates are realised as 1-out-of-n threshold gates}, and the leaves containing attributes. Decryption of the message is then allowed when the attributes satisfy the tree. This access structure, called the policy, is stored in the private key. In the original model, \textit{all} attributes had to match approximately (within the predefined set distance), so this new access structure is much more expressive. \Gls{KP-ABE} associates the ciphertexts with these descriptive attributes, and the user's keys with the policy.

\input{images/analysis/policy}

\noindent This scheme works similar to secret sharing schemes, where a number of parties (respectively, attributes) must collude to be able to reconstruct the secret. The main difference is that secret sharing allows cooperation between different parties, but \gls{KP-ABE} does not. If this were allowed, some security requirements could be bypassed. This property is called \textit{collusion resistance}, and is an essential property of \gls{ABE} systems \citep{cp-abe}. 

As an example, take the case of government health data that is encrypted with the attributes \texttt{government} \texttt{AND} \texttt{health}. You would not want a government worker with attribute \texttt{government} be able to collude with a nurse with attribute \texttt{health} to decrypt this data.

\acrlong{CP-ABE} then, is introduced by \citet{cp-abe}. \Gls{CP-ABE} reverses the situation of \gls{KP-ABE}, by associating the private keys with descriptive attributes, and the ciphertexts with the access structure also called the policy). Because of this, the secret sharing technique can no longer be used and \citeauthor{cp-abe} introduce a new private key randomization technique. 

The table below highlights the main conceptual differences between \gls{KP-ABE} and \gls{CP-ABE}:
\input{tables/abe-comparison}

\noindent In the context of Solid and a privacy-enhancing middleware, \gls{ABE} provides a number of useful applications. The transformation-based approaches in section \ref{sec:transformation-approaches} protect against the threat of untrusted applications. \Gls{ABE}, on the other hand, could be interesting to use when the Solid pod is not trusted completely. Extremely sensitive data (such as doctors reports or police case files) could then be stored on Solid pods, guaranteeing data security even when the Pod is compromised or access control systems are bypassed.

Conceptually, the user in \gls{ABE} schemes then maps to a Solid application, while the encrypted data is stored at the Solid pod. The (Solid) user can then specify which applications map to which attributes, while simultaneously deciding which access structures should be associated with a Solid resource. Because of this association, \gls{CP-ABE} is the most suitable type of \gls{ABE} to use in such a middleware. Investigating \gls{ABE} as a mechanism for realizing sharing of sensitive data within Solid is out of scope for this thesis, so researching possible (attribute-based) encryption techniques in this context may be interesting future research. This is discussed in section \ref{sec:future-work} under FW \ref{fw:abe}.
