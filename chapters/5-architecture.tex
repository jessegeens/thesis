\chapter{Architecture and design}
\section{Design decisions}
In every architectural design, important decisions have to be made based on some sort of cost-benefit analysis. Similarly, this is the case with the design \middleware{}. This section describes the reasoning behind a number of important design decisions that have been made during the modelling and development.

\subsection{Integration}
A first important aspect of the design of \middleware{} is where it is logically positioned. There are a number of possibilities, both with their own advantages. 

One possibility would be to make \middleware{} an extension of an existing Solid Server. This would result in significantly sped up development, as well as much better performance. Despite that, this approach has a major issue, namely its centralization. Decentralization is a core principle of Solid, and such a design would be very centralized in two ways. First of all, integrating into an existing Solid server makes the solution incompatible with other existing Solid servers. This also limits its potential usability because only a subset of users can use \middleware{}. Secondly, this would limit usability for users who use a Pod provided by a third party. In such a case, if the third party does not have a Solid server with the \middleware{} extension running, then the user is unable to use this technology. 

The other possibility for realising the goals and requirements that were laid out, would be to build \middleware{} as a proxy. Requests from the Solid application would be directed to \middleware{} instead, which fetches the data, sanitizes it, and then replies with the sanitized data. This is a much less centralized option, but comes at a big performance cost. There are is more network usage and double the number of HTTP requests, for example. 

Given that centralization is such a core concept to Solid, the second option is believed to be the better option in this context. The performance cost does not outweigh the decentralization gain.

\subsection{Supporting multiple data schemes}
Solid supports storing nearly any type of resource, both linked and non-linked. The data that is prone to leakage and that will be most likely requested by applications is structured data. Unstructured data may also contain \gls{PII}, but this is discussed in section \ref{sec:limitations}. Structured data can be many forms, and any good middleware should be independent of the types of structured data that are requested or passing through it. However, \middleware{} needs to perform operations on the data that passes through it, and should thus have some context as to how it should handle a specific data scheme. Two possibilities for tackling this problem were envisioned.

A first possibility would be to have separate components, where each component is responsible for performing transformations on the input data of a specific data scheme. The components would be loaded dynamically at run-time, using dependency injection technology such as \textit{components.js}\footnote{\url{https://componentsjs.readthedocs.io/en/latest/}}. This would result in great flexibility: the transformation component can perform virtually any transformation. However, the big downside of this strategy is that it results in a significant development time for each component (and thus, for every supported data scheme). This can result in a lack of support for many (popular) data schemes, making \middleware{} significantly less useful. 

A second option that was considered was to provide \middleware{} with an internal library of "transformation components", one per content representation. For example, there would be a component that performs pseudonymization on data that is represented in JSON. At startup, the software would then read a number of \textit{scheme configuration files}, one for every supported data scheme. These files then specify how a data scheme should be detected, and what transformations ought to be applied to it. This requires more up-front coding to support this library of transformations for content representations. It also limits flexibility: only data schemes for which the content representation is supported can be handled, and transformations are limited to those that are supported in the middleware. Furthermore, there is also the performance cost of having to interpret these files instead of directly executing code. Despite these drawbacks, there is also a very large advantage to this approach: supporting new data schemes becomes a much easier and faster task. The only work that needs to be performed to support a new data scheme would be to write a scheme configuration file, something that can be done in a few hundred lines of JSON.

In the end, the second option was opted for, for a pragmatic reason. Having only a limited number of transformations but supporting much more data schemes results in a middleware that can have a bigger privacy impact than one that does slightly better transformations, but on a much more limited number of data schemes (and nothing on the ones that aren't supported).

\subsection{Domain knowledge-dependent transformations}
\todo[inline]{Some transformations (eg generalizing strings) require domain knowledge (eg "Spar" is a type of "Supermarket"). How to store this domain knowledge?}

\section{Design}
\todo[inline]{Give general introduction to the architecture here, such as a system-level overview, specify the big flow of the software (eg what happens when a user makes a request to the proxy). The complete model from VP can maybe be added in the appendix? (using the SAPlugin for TeX generation)}

\subsection{Pluggable data format transformations}
\todo[inline]{Rewrite this in the design decisions section to explain the two considered possibilities and why the second choice was opted for}
Right now, considering two options for allowing the "pluggable design" of the supported data formats. 
\begin{enumerate}[label=(\alph*)]
\item First option: there are separate "components" for transforming the data of a single file type. The data goes through the file type detector to select a "data type". Then, in some config file (eg datatypes.json), there is a mapping of "datatype" -$>$ "transformerComponent" (using eg components.js or something similar for another language). The advantage is here that any arbitrary code can be used to do some data transformations, and any file format can be used, even the most exotic ones. The disadvantage is that it takes more effort to code extra supported data types, as a whole transformer must be coded.
\item Second option: writing transformators for the data is done by some sort of config files. See the listing below for an example. In this approach, there are a number of data transformations provided by the middleware (eg a component for removing an attribute, one for generalizing, one for generating a pseudonym etc). There is one of each for every file type (eg a component to remove an attribute from an xml document, one to generalize a field in a JSON doc, etc). The disadvantages are that this requires more up-front coding in the middleware to support all these transformations and file formats (eg JSON, XML, TTL), and limits extensibility to other file formats / transformations since these need to be coded into the middleware. This could also be made pluggable but then it may become a mess and become hard to make sense of (maybe in a future stage? but to start with, seems very complex). On the other hand, this does make it much more easy to support new data types, since you just specify which transformations to apply to which fields, such as in the example below
\end{enumerate}

\begin{listing}
\begin{minted}[linenos,tabsize=2,breaklines]{json}
{
	"schemeName": "TCXActivity",
    "detector": {
    	"contentRepresentation": "xml",
        "scheme": "https://www8.garmin.com/xmlschemas/TrainingCenterDatabasev2",
        "mechanism": {
        	"mechanismName": "filenameContains",
            "value": ".tcx"
        }
    },
    "transformations":[
      {
      	"level": 2,
        "tactics": [{
        	"field": "AverageHeartRateBpm",
			"fieldType": "integer",
			"transformation": {
            	"transformationName": "remove"
            }
        }]
      },
      {
      	"level": 3,
        "tactics": [{
        	"field": "AverageHeartRateBpm",
          	"fieldType": "integer",
          	"transformation": {
            	"transformationName": "remove"
            }
        },
        {
        	"field": "Cadence",
          	"fieldType": "integer",
          	"transformation": {
            	"transformationName": "pseudonymization",
              	"pseudonym": "100"
            }
        }]
      }
    ]
}
\end{minted}
\caption{An example scheme configuration file}
\end{listing}


\section{Implementation}
\todo[inline]{Write here about concrete implementation: eg, what language used and why, how does dynamic plugin loading happen (eg using components.js or something similar), which hurdles had to be overcome during the implementation etc
-> maybe make notes here about problems on the road so you don't forget them}

\section{Limitations}
\label{sec:limitations}
\todo[inline]{Describe here what the limitations of the proposed middleware are: what functionality does it lack? Under what attacker models is it insecure? What other limitations are there (eg certain things cannot be supported? it is not compliant with the spec in some cases? Maybe give a hint to future work here already}
\todo[inline]{Right now, just some notes}
\begin{itemize}
    \item High performance overhead compared to a solution integrated into the Solid Community Server
    \item Does not support writing back/updating resources, as this would require to keep a mapping when pseudonymization is applied for example -> would need to drastically extend the architecture
    \item Currently, only protects privacy on the basis of a single resource/dataset. Very often, de-anonymization happens because multiple datasets are linked together; protection here is more limited since \middleware{} transforms resources on a one-by-one basis without looking at the total. Of course, not all is lost, because often direct identifiers are removed if the correct privacy level is selected, making it much harder to link datasets together.
    
\end{itemize}